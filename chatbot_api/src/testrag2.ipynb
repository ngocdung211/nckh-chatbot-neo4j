{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅✅call agent step\n",
      "get graph\n",
      "get llm \n"
     ]
    }
   ],
   "source": [
    "from agents.rag_agent import chat_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chains.semantic_search_chunk_chain import chunk_retriever\n",
    "from chains.semantic_search_chunk_chain import get_chunk_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"testset_5files.csv\")\n",
    "question = df['user_input']\n",
    "ground_truth = df['reference']\n",
    "retrieval_context = df['reference_contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"content: BẠN CHẮP HAlẸ’iẾH’lỉ TRUNGƯƠNG ĐOÀN TNCS HỎ CHÍ MINH Số: 4244-QD/TWĐTN-TNTH Hà Nội, ngàylÿ tháng ÁQ năm 2019 - (2UYET ĐỊNH Về việc ban hành Quy chế giải thưởng “/Vhä giáo trẻ tiêu biễu? uốp Trung ương BẠN BÍ THƯ TRUNG ƯƠNG ĐOÀN TNCS HỖ CHÍ MINH - Căn cứ Điều 31, Chương IX, Điều lệ Đoàn TNCS Hỗ Chí Minh; - Căn cứ Quy chề hoạt íz'ọng của Ban Chấp hành 7ì Tung ương Đoàn TNCS Hồ Chí Minh khoá XI, Quy chế thì đua, .khen thưởng của Đoàn TNCS Hồ Chí Miình ban hành kèm theo Quyết định số 1 09-QĐ/TWĐTN-VP ngày 15/8/2018 của Ban Thường vụ Trung ương Đoàn TNCS Hồ Chí Minh; _ - Xét đề nghị của Ban Thanh niên Trường học Trung ương Đoàn, QUYẾT ĐỊNH Điều 1. Ban hành kèm theo Quyết định mty Quy chế Ẻai thưởng “Nhà giáo trẻ tiêu biểu” cấp Trung ương dành cho giáo viên, giảng viên trẻ đang công tác, giảng dạy trong các trường, cơ sở giáo , dục đào tạo, cơ sở giáo dục nghề nghiệp thuộc hệ thống giáo dục quoc dân. Điều 2. Ban Thanh niên Trường học Trung ương Đoàn là cơ quan thường - trực giải thưởng, có trách nhiệm tham mưu Ban Bí thư Trung ương Đoàn chỉ đạo tốổ chức thực hiện việc trao tặng giải thưởng “Vhà giáo trẻ tiêu biểu? cấp Trung ương hàng năm. Điều 3. Quyết định này có hiệu lực kể từ ngày ký. Điều 4. Ban Thanh niên Trường học, Văn phòng, các ban, đơn vị trực thuộc Trung tIơng Đoàn, các tỉnh, thành đoàn, đoàn trực thuộc căn cử quyết định thi hành. Nơi nhận: ; TM. BẠN BÍ THƯ TRUNG ƯƠNG ĐOÀN ''“-Như điển 4; BỈ THƯ 1ỄHỨ NĨ-Í.a’ARẵ'.Í` - Ban Dân vận TW, Ban Tuyên giáo TW, Văn phòng TW, Ban\\nnext_contents:\\nprev_contents:\\n- . Điều 4. Ban Thanh niên Trường học, Văn phòng, các ban, đơn vị trực thuộc Trung tIơng Đoàn, các tỉnh, thành đoàn, đoàn trực thuộc căn cử quyết định thi hành. Nơi nhận: ; TM. BẠN BÍ THƯ TRUNG ƯƠNG ĐOÀN ''“-Như điển 4; BỈ THƯ 1ỄHỨ NĨ-Í.a’ARẵ'.Í` - Ban Dân vận TW, Ban Tuyên giáo TW, Văn phòng TW, Ban Thị đua - Khen thưởng TW (để báo cáo); - Các Bộ, ngành, cơ quan Trung ương; - Ban Bí thư TW Đoàn; - BTV các tỉnh, thành đoàn, đoàn trực thuộc; - Các cơ quan thông tấn, ủ’ìo chí, tổ chức xã hội; -LưuTNTH,VP.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chunk_retriever().invoke(\"ĐOÀN TNCS HỒ CHÍ MINH là gì?\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "for query in question:\n",
    "  answers.append(chat_agent.invoke(\n",
    "    {\"input\": query},\n",
    "    {\"configurable\": {\"session_id\": 1}}).get(\"output\", \"No output text provided.\"))\n",
    "  contexts.append([docs.page_content for docs in get_chunk_retriever().invoke(query)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"response\"] = answers\n",
    "df[\"retrieved_contexts\"] = contexts\n",
    "df.to_csv(\"test_set_rag_13-12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "data = {\n",
    "    \"user_input\": question,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"reference\": df['reference'],\n",
    "    \"reference_contexts\": df[\"reference_contexts\"].apply(ast.literal_eval)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "# from ragas import \n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv('test_set_rag_13-12.csv')\n",
    "df['retrieved_contexts'] = df['retrieved_contexts'].apply(ast.literal_eval)\n",
    "df['reference_contexts'] = df['reference_contexts'].apply(ast.literal_eval)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "# dataset= Dataset.from_dict(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 60/60 [01:37<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    token_usage_parser=get_token_usage_for_openai,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=275559 output_tokens=38709 model=''\n"
     ]
    }
   ],
   "source": [
    "print(result.total_tokens())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method EvaluationResult.total_tokens of {'context_precision': 0.9611, 'context_recall': 0.7844, 'faithfulness': 0.6798, 'answer_relevancy': 0.8966}>\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"final_result_13-12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get llm \n",
      "get_embedding_function\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from llm.get_llm import get_eval_model_function, get_embedding_function\n",
    "evaluator_llm = LangchainLLMWrapper(get_eval_model_function())\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(get_embedding_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|█████▏    | 31/60 [02:58<03:43,  7.69s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Evaluating:  57%|█████▋    | 34/60 [03:01<01:38,  3.78s/it]Exception raised in Job[18]: TimeoutError()\n",
      "Evaluating:  58%|█████▊    | 35/60 [03:02<01:19,  3.16s/it]Exception raised in Job[22]: TimeoutError()\n",
      "Evaluating:  62%|██████▏   | 37/60 [03:06<01:03,  2.78s/it]Exception raised in Job[26]: TimeoutError()\n",
      "Evaluating:  68%|██████▊   | 41/60 [03:15<00:40,  2.14s/it]Exception raised in Job[28]: TimeoutError()\n",
      "Evaluating:  72%|███████▏  | 43/60 [03:38<02:02,  7.19s/it]Exception raised in Job[32]: TimeoutError()\n",
      "Evaluating:  77%|███████▋  | 46/60 [04:08<02:05,  8.97s/it]Exception raised in Job[34]: TimeoutError()\n",
      "Evaluating:  88%|████████▊ | 53/60 [04:44<00:39,  5.68s/it]Exception raised in Job[38]: TimeoutError()\n",
      "Evaluating:  95%|█████████▌| 57/60 [05:21<00:22,  7.43s/it]Exception raised in Job[42]: TimeoutError()\n",
      "Evaluating:  97%|█████████▋| 58/60 [05:28<00:14,  7.19s/it]Exception raised in Job[44]: TimeoutError()\n",
      "Evaluating:  98%|█████████▊| 59/60 [05:46<00:10, 10.36s/it]Exception raised in Job[46]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 60/60 [05:58<00:00,  5.97s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_token_usage_for_openai\n\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     LLMContextRecall(llm\u001b[38;5;241m=\u001b[39mevaluator_llm), \n\u001b[1;32m      5\u001b[0m     FactualCorrectness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_usage_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_token_usage_for_openai\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test_ragas3/lib/python3.11/site-packages/ragas/_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test_ragas3/lib/python3.11/site-packages/ragas/evaluation.py:333\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# evalution run was successful\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# now lets process the results\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     cost_cb \u001b[38;5;241m=\u001b[39m ragas_callbacks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_cb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ragas_callbacks \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinary_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCostCallbackHandler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcost_cb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mragas_traces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    345\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mscores})\n",
      "File \u001b[0;32m<string>:10\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, scores, dataset, binary_columns, cost_cb, traces, ragas_traces, run_id)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test_ragas3/lib/python3.11/site-packages/ragas/dataset_schema.py:410\u001b[0m, in \u001b[0;36mEvaluationResult.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# parse the traces\u001b[39;00m\n\u001b[1;32m    409\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraces \u001b[38;5;241m=\u001b[39m \u001b[43mparse_run_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mragas_traces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test_ragas3/lib/python3.11/site-packages/ragas/callbacks.py:167\u001b[0m, in \u001b[0;36mparse_run_traces\u001b[0;34m(traces, parent_run_id)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, prompt_uuid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metric_trace\u001b[38;5;241m.\u001b[39mchildren):\n\u001b[1;32m    164\u001b[0m         prompt_trace \u001b[38;5;241m=\u001b[39m traces[prompt_uuid]\n\u001b[1;32m    165\u001b[0m         prompt_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_trace\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[0;32m--> 167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mprompt_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    168\u001b[0m         }\n\u001b[1;32m    169\u001b[0m     metric_traces[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_trace\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_traces\n\u001b[1;32m    170\u001b[0m parased_traces\u001b[38;5;241m.\u001b[39mappend(metric_traces)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from ragas.cost import get_token_usage_for_openai\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "    \n",
    "]\n",
    "results = evaluate(dataset=dataset, metrics=metrics, token_usage_parser=get_token_usage_for_openai,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_tokens()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_ragas3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
